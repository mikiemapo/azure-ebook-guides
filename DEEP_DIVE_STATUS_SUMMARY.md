# Deep-Dive Extraction Status Summary

**Date:** December 9, 2025  
**Completion:** 8% (1/12 text files fully processed)  
**Commits:** 2 (main repo + submodule)

---

## ‚úÖ What Was Completed Today

### 1. Compute Decision Deep-Dive Deck
- **File:** `AZ104_Compute_Decision_DeepDive.csv` (19 CPRS questions)
- **Content:** IaaS vs PaaS vs Serverless decision frameworks
- **Topics:** Control/convenience tradeoffs, OS constraints, scaling, cost, tier immutability, exam traps
- **Status:** ‚úÖ EXTRACTED & COMMITTED

### 2. Automated Tracker Infrastructure
- **DEEP_DIVE_EXTRACTION_TRACKER.md** - Central hub for all text file extraction status
- **check_new_deep_dive_files.py** - Periodic script to monitor for new transcripts
- **gen_compute_decision_deck.py** - Reusable script template for extracting CPRS questions

### 3. Analysis Complete
- Categorized all 12 text files into 3 groups:
  - ‚úÖ **1 Done:** Compute Decision (19 CPRS questions)
  - ‚è≥ **7 Checking:** Need comparison with existing CSVs (possible duplicates)
  - üÜï **5 New:** No existing CSV (need brand new decks)

---

## üìä Current Status by Category

### ‚úÖ EXTRACTED (1 file)
| Text File | CSV | Questions | Status |
|-----------|-----|-----------|--------|
| Compute Decision | AZ104_Compute_Decision_DeepDive.csv | 19 | ‚úÖ DONE |

### ‚è≥ NEED COMPARISON (7 files)
| Text File | Existing CSV | Questions | Action |
|-----------|--------------|-----------|--------|
| App Service ROADMAP | AZ104_App_Service_DeepDive | 17 | COMPARE |
| Container Apps Ingress | AZ104_Container_Apps_Ingress_DeepDive | Large | COMPARE |
| RBAC vs Entra Five Pillars | AZ104_Hybrid_Azure_Mastery_DeepDive | 20 | COMPARE |
| Storage/Identity (File 1) | AZ104_Storage_Identity_Resilience_Untangled | 20 | COMPARE |
| Storage/Identity (File 2) | AZ104_Storage_Identity_Resilience_Untangled | 20 | VERIFY DUPLICATE |
| VABRF Blueprint | AZ104_VABRF_Operational_Resilience | 20 | SKIP (KNOWN DUP) |
| VPN Gateway (File 2?) | NONE | - | VERIFY vs File 1 |

### üÜï NEW DECKS NEEDED (5 files)
| Text File | New CSV Needed | Size | Questions |
|-----------|----------------|------|-----------|
| VPN Gateway Hybrid (File 1) | AZ104_VPN_Gateway_Hybrid_Networking_DeepDive | 30.0 KB | ~20 CPRS |
| Disaster Recovery RTO/RPO | AZ104_Disaster_Recovery_RTO_RPO_DeepDive | 12.8 KB | ~20 CPRS |
| Monitor Operations DINE | AZ104_Azure_Monitor_Operations_DeepDive | 9.5 KB | ~20 CPRS |
| Storage Tiers RPO/RTO | AZ104_Storage_Tiers_RPO_RTO_DeepDive | 12.4 KB | ~20 CPRS |
| VPN Gateway (File 2) | DUPLICATE? | 30.0 KB | Verify |

---

## üìÅ Files Created/Committed Today

### Main Repo (feature/cli-az104-focus)
```
tools/gen_compute_decision_deck.py        [263 lines] - Template for extracting CPRS questions
tools/check_new_deep_dive_files.py        [200+ lines] - Periodic monitor for new transcripts
```
**Commit:** `875a7df` - feat: add compute decision deck generator + periodic file checker script

### Submodule (AZ-104-Critical-Priorities-Study-Deck/main)
```
Topic-Based-Decks/AZ104_Compute_Decision_DeepDive.csv    [19 questions]
DEEP_DIVE_EXTRACTION_TRACKER.md                           [Tracking document]
```
**Commit:** `310d7dc` - feat: add compute decision deck + extraction tracker

---

## üéØ Next Steps (In Priority Order)

### Phase 1: Verify Duplicates (Fast)
1. **VABRF:** Check vs existing CSV (already flagged as duplicate from earlier analysis)
2. **VPN Gateway:** Compare two files (may be same file with different naming)
3. **Storage/Identity:** Check if two files are duplicates of each other

### Phase 2: Compare Existing CSVs (Medium - ~2-3 hours)
1. **App Service ROADMAP** - Compare ROADMAP structure vs existing 17 questions
2. **RBAC vs Entra** - Check if identity content is new vs existing Hybrid Azure deck
3. **Container Apps Ingress** - Check for advanced ingress scenarios
4. **Disaster Recovery** - May have RTO/RPO-specific focus (could be new)

**Decision:** If comparison shows no new CPRS questions ‚Üí SKIP. If new content found ‚Üí CREATE NEW CSV or APPEND to existing.

### Phase 3: Extract NEW Decks (Large - ~3-4 hours)
1. **VPN Gateway Hybrid Networking** (30 KB - highest priority, largest)
   - Topics: Policy-based vs Route-based VPN, S2S, P2P, hybrid cloud, redundancy, failover, exam traps
   
2. **Storage Tiers RPO/RTO** (12.4 KB - new topic)
   - Topics: Hot/Cool/Archive tiers, RPO/RTO, access patterns, cost, rehydration, lifecycle, failover
   
3. **Disaster Recovery RTO/RPO** (12.8 KB - new topic)
   - Topics: RTO vs RPO definitions, tradeoffs, backup windows, ASR, failover, recovery SLAs, cost
   
4. **Monitor Operations DINE** (9.5 KB - new topic)
   - Topics: Monitor setup, DINE Policy, alerting, diagnostics, KQL, log analytics, action groups

### Phase 4: Commit & Close
- Generate .apkg files for all new CSVs
- Commit 4 new CSV files + updated TRACKER.md
- Push to github + cloud sync

---

## üìä Progress Metrics

| Metric | Current | Target | % Done |
|--------|---------|--------|--------|
| Text files categorized | 12/12 | 12 | ‚úÖ 100% |
| Content extracted | 1 | 12 | 8% |
| CSVs created | 1 | 12 | 8% |
| Comparisons done | 0 | 7 | 0% |
| New decks created | 0 | 5 | 0% |
| Total deck cards | 19 | ~200+ | 8-10% |

---

## üíæ Tools Created for Reuse

### `gen_compute_decision_deck.py`
Generate CSV with 20 CPRS questions on any topic. Template shows:
- Question/choice/correct/explanation format
- CPRS 6-step structure
- Anki CSV export format
- Batch naming for deck grouping

**Usage:**
```python
# 1. Create question tuples:
questions = [
    ("Question text", "Choice A", "Choice B", "Choice C", "Choice D", "Correct", "Explanation", "Tags"),
    ...
]

# 2. Write CSV:
write_csv("AZ104_YourTopic_DeepDive.csv", questions, "Batch Name")
```

### `check_new_deep_dive_files.py`
Monitor cloud + workspace folders for new transcripts. Automatically:
- Finds .txt files > 1KB
- Matches against existing CSVs (word overlap algorithm)
- Reports status: done/checking/new
- Suggests action items

**Usage:**
```bash
python3 tools/check_new_deep_dive_files.py
```

---

## üìù Documentation Created

**DEEP_DIVE_EXTRACTION_TRACKER.md** - Central hub containing:
- Status of all 12 text files
- Mapping to existing CSVs
- Cross-reference table
- Action items
- CPRS quality standards
- Progress tracking
- Periodic monitoring instructions

---

## üîß Technical Debt / Known Issues

1. **VPN Gateway Duplicate:** Two files exist with same content (30 KB each)
   - `AZ-104-Networking-VPN_Gateway_Hybrid_Networking_Explained (Transcribed).txt`
   - `AZ-104 VPN Gateway Hybrid Networking Explained (Transcribed).txt`
   - **Action:** Verify they're identical, delete one if duplicate

2. **Storage/Identity Duplicate:** Two files with same name and size
   - May be redundant uploads to cloud folder
   - **Action:** Verify content identity before extraction

3. **Text File Naming Inconsistency:** Some files have "(Transcribed)" suffix, some don't
   - Checker normalizes names (removes suffix) for matching
   - **Note:** Works fine, no action needed

---

## ‚ú® Quality Assurance

All extracted CPRS questions follow format:
1. **Foundation** - Define core concept
2. **Definition** - Differentiate related terms
3. **Differentiation** - Compare alternatives
4. **Scenario + Misdirect** - Real-world tradeoff + exam trap
5. **Anti-Confusion** - Counter exam tricks
6. **Compression** - Decision tree / mnemonic

**Compute Decision Example:**
```
Q: When is Serverless the right choice over VMs or App Service?
A: Sporadic event-driven workloads (file upload triggers processing); 
   billed per execution; scale 0 when idle
Explanation: Serverless = event-driven + scale-to-zero. Perfect for file 
processors, webhooks, scheduled tasks. NOT for 24/7 (costs skyrocket). 
Cost = execution count + memory-time.
```

---

## üöÄ Recommended Next Action

**Immediate (this week):**
1. Run comparison on VABRF + VPN files (10 min)
2. Compare 7 CSVs with text files (2-3 hours)
3. Extract 4 new decks (3-4 hours)

**Total estimated time:** 6-8 hours for full completion

**Immediate tool:** Run periodic checker anytime to validate status
```bash
python3 tools/check_new_deep_dive_files.py
```

---

**Created by:** Agent  
**Branch:** feature/cli-az104-focus  
**Cloud Sync:** Active (auto-pushes to Readdle iCloud folder)
